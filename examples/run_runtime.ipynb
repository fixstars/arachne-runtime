{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime: Run Your Model\n",
    "To test converted or compiled models, arachne has the runtime that wraps original runtimes.  \n",
    "Currently, the arachne runtime supports the onnx, tflite, and tvm model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and compile a model\n",
    "We compile mobilenet tf.keras model in tvm. Here tvm_target is set to the CPU as default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.applications.mobilenet.MobileNet()\n",
    "model.save(\"mobilenet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model input and output information as yaml format for arachne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yml = \"\"\"\n",
    "inputs:\n",
    "  - dtype: float32\n",
    "    name: input\n",
    "    shape:\n",
    "    - 1\n",
    "    - 224\n",
    "    - 224\n",
    "    - 3\n",
    "outputs:\n",
    "  - dtype: float32\n",
    "    name: output\n",
    "    shape:\n",
    "    - 1\n",
    "    - 1000\n",
    "\"\"\"\n",
    "open(\"mobilenet.yml\", \"w\").write(yml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m arachne.driver.cli \\\n",
    "    +tools=tvm \\\n",
    "    model_file=./mobilenet.h5 \\\n",
    "    output_path=./mobilenet.tar \\\n",
    "    model_spec_file=./mobilenet.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on local\n",
    "Run compiled tvm model using `arachne.runtime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285: 'Egyptian cat',\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tvm.contrib.download import download\n",
    "\n",
    "import arachne.runtime\n",
    "import arachne.runtime.rpc\n",
    "import arachne.tools.tvm\n",
    "import cv2\n",
    "\n",
    "def get_input_data():\n",
    "    image_url = \"https://arachne-public-pkgs.s3.ap-northeast-1.amazonaws.com/data/cat.jpg\"\n",
    "    image_path = \"cat.jpg\"\n",
    "    download(image_url, image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img - np.array([123.0, 117.0, 104.0])\n",
    "    img /= np.array([58.395, 57.12, 57.375])\n",
    "    input_data = img[np.newaxis, :, :, :].astype(np.float32)  # type: ignore\n",
    "    return input_data\n",
    "\n",
    "def imagenet_1000_class(idx):\n",
    "    url = \"https://gist.githubusercontent.com/zhreshold/4d0b62f3d01426887599d4f7ede23ee5/raw/596b27d23537e5a1b5751d2b0481ef172f58b539/imagenet1000_clsid_to_human.txt\"\n",
    "    path = \"imagenet_1000.txt\"\n",
    "    download(url, path)\n",
    "    return open(path).readlines()[idx].strip()\n",
    "\n",
    "# Init runtime by the tar files that arachne.tools output\n",
    "runtime_module = arachne.runtime.init(runtime=\"tvm\", package_tar=\"mobilenet.tar\")\n",
    "# or init runtime by specific model files and environment files\n",
    "# runtime_module = arachne.runtime.init(runtime=\"tvm\", model_file=\"mobilenet.tar\", env_file=\"env.yaml\")\n",
    "\n",
    "# Set an input\n",
    "input_data = get_input_data()\n",
    "runtime_module.set_input(0, input_data)\n",
    "# Run an inference\n",
    "runtime_module.run()\n",
    "# Get a result\n",
    "out = runtime_module.get_output(0)\n",
    "print(imagenet_1000_class(np.argmax(out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPC: Run Your Model on Remote devices\n",
    "\n",
    "`arachne.runtime.rpc` provides remote execution function on a device using RPC (remote procedure call).  \n",
    "We describe an example of running mobilenet on jetson-xavier-nx.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile a model for edge device\n",
    "Compile the mobilenet model same as previous step for the edge device. We set `jetson-xavier-nx` to `tvm_target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-22 04:24:12.703818: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-22 04:24:20.632856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 373 MB memory:  -> device: 0, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.636986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30554 MB memory:  -> device: 1, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.639957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30554 MB memory:  -> device: 2, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.642426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14349 MB memory:  -> device: 3, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:0b:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.644931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 14349 MB memory:  -> device: 4, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.647407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 13059 MB memory:  -> device: 5, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.649959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 15003 MB memory:  -> device: 6, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2022-04-22 04:24:20.653148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 30556 MB memory:  -> device: 7, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[2022-04-22 04:24:21,305][tensorflow][WARNING] - No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[2022-04-22 04:24:22,064][tensorflow][WARNING] - No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[04:24:22] /workspaces/arachne/3rdparty/tvm/src/relay/transforms/convert_layout.cc:99: Warning: Desired layout(s) not specified for op: nn.global_avg_pool2d\n",
      "call_node: \n",
      "free_var %input_1: Tensor[(1, 224, 224, 3), float32];\n",
      "%0 = @tvmgen_default_tensorrt_main_0(%input_1) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "%1 = (%0,);\n",
      "%2 = call_lowered(@tvmgen_default_fused_nn_pad, %1, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"9b6e21c5ba237b72\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad']}) /* ty=Tensor[(1, 64, 113, 113), float32] */;\n",
      "%3 = @tvmgen_default_tensorrt_main_16(%2) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
      "%4 = (%3,);\n",
      "%5 = call_lowered(@tvmgen_default_fused_nn_pad_1, %4, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"6adbfa7f938f1a1c\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad_1']}) /* ty=Tensor[(1, 128, 57, 57), float32] */;\n",
      "%6 = @tvmgen_default_tensorrt_main_37(%5) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
      "%7 = (%6,);\n",
      "%8 = call_lowered(@tvmgen_default_fused_nn_pad_2, %7, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"4e8567882dec655f\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad_2']}) /* ty=Tensor[(1, 256, 29, 29), float32] */;\n",
      "%9 = @tvmgen_default_tensorrt_main_58(%8) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
      "%10 = (%9,);\n",
      "%11 = call_lowered(@tvmgen_default_fused_nn_pad_3, %10, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"9a7cae363e20eb81\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad_3']}) /* ty=Tensor[(1, 512, 15, 15), float32] */;\n",
      "@tvmgen_default_tensorrt_main_119(%11) /* ty=Tensor[(1, 1000), float32] */\n",
      "call_node: \n",
      "free_var %input_1: Tensor[(1, 224, 224, 3), float32];\n",
      "%0 = @tvmgen_default_tensorrt_main_0(%input_1) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "%1 = (%0,);\n",
      "%2 = call_lowered(@tvmgen_default_fused_nn_pad, %1, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"9b6e21c5ba237b72\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad']}) /* ty=Tensor[(1, 64, 113, 113), float32] */;\n",
      "%3 = @tvmgen_default_tensorrt_main_16(%2) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
      "%4 = (%3,);\n",
      "%5 = call_lowered(@tvmgen_default_fused_nn_pad_1, %4, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"6adbfa7f938f1a1c\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad_1']}) /* ty=Tensor[(1, 128, 57, 57), float32] */;\n",
      "%6 = @tvmgen_default_tensorrt_main_37(%5) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
      "%7 = (%6,);\n",
      "%8 = call_lowered(@tvmgen_default_fused_nn_pad_2, %7, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"4e8567882dec655f\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad_2']}) /* ty=Tensor[(1, 256, 29, 29), float32] */;\n",
      "@tvmgen_default_tensorrt_main_58(%8) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
      "call_node: \n",
      "free_var %input_1: Tensor[(1, 224, 224, 3), float32];\n",
      "%0 = @tvmgen_default_tensorrt_main_0(%input_1) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "%1 = (%0,);\n",
      "%2 = call_lowered(@tvmgen_default_fused_nn_pad, %1, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"9b6e21c5ba237b72\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad']}) /* ty=Tensor[(1, 64, 113, 113), float32] */;\n",
      "%3 = @tvmgen_default_tensorrt_main_16(%2) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
      "%4 = (%3,);\n",
      "%5 = call_lowered(@tvmgen_default_fused_nn_pad_1, %4, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"6adbfa7f938f1a1c\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad_1']}) /* ty=Tensor[(1, 128, 57, 57), float32] */;\n",
      "@tvmgen_default_tensorrt_main_37(%5) /* ty=Tensor[(1, 256, 28, 28), float32] */\n",
      "call_node: \n",
      "free_var %input_1: Tensor[(1, 224, 224, 3), float32];\n",
      "%0 = @tvmgen_default_tensorrt_main_0(%input_1) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
      "%1 = (%0,);\n",
      "%2 = call_lowered(@tvmgen_default_fused_nn_pad, %1, metadata={\"relay_attrs\"={__dict__={\"Primitive\"=1, \"hash\"=\"9b6e21c5ba237b72\"}}, \"all_prim_fn_vars\"=['tvmgen_default_fused_nn_pad']}) /* ty=Tensor[(1, 64, 113, 113), float32] */;\n",
      "@tvmgen_default_tensorrt_main_16(%2) /* ty=Tensor[(1, 128, 56, 56), float32] */\n",
      "call_node: \n",
      "free_var %input_1: Tensor[(1, 224, 224, 3), float32];\n",
      "@tvmgen_default_tensorrt_main_0(%input_1) /* ty=Tensor[(1, 64, 112, 112), float32] */\n"
     ]
    }
   ],
   "source": [
    "!python -m arachne.driver.cli \\\n",
    "    +tools=tvm \\\n",
    "    +tvm_target=jetson-xavier-nx \\\n",
    "    model_file=./mobilenet.h5 \\\n",
    "    output_path=./mobilenet_edge.tar \\\n",
    "    model_spec_file=./mobilenet.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start RPC server\n",
    "\n",
    "Please refer to the [Device: Setup device](https://arachne.gitlab.fixstars.com/arachne/tutorials/runtime.html) for device environment setup.\n",
    "Start the rpc server using the created venv and arachne: `./setup.sh <env_dirname> <runtime_name> <port>`.\n",
    "You can specify either tvm, tflite, onnx to `<runtime_name>`.\n",
    "The following example shows the TVM runtime server running on JetPack 4.6 on port 5051.\n",
    "\n",
    "```sh\n",
    "cd arachne/device\n",
    "./setup.sh jp46 tvm 5051\n",
    "```\n",
    "\n",
    "Or, you can also start server as the following:  \n",
    "\n",
    "```sh\n",
    "cd arachne/device\n",
    "source jp46/.venv/bin/activate\n",
    "python -m arachne.runtime.rpc.server --port 5051 --runtime tvm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on remote devices\n",
    "\n",
    "To perform remote execution via RPC, initialize RuntimeClient with `arachne.runtime.rpc.init`.  \n",
    "RuntimeClient has the same methods as RuntimeModule, and requests data i/o and model execution to the RuntimeServer running on the edge device.  \n",
    "RuntimeServicer has `arachne.runtime.RuntimeModule` instance internally and provides model execution services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify it to your environment\n",
    "rpc_host = \"192.168.xx.yy\"\n",
    "rpc_port = 5051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285: 'Egyptian cat',\n"
     ]
    }
   ],
   "source": [
    "runtime_module = arachne.runtime.rpc.init(\n",
    "    runtime=\"tvm\",\n",
    "    package_tar=\"mobilenet_edge.tar\",\n",
    "    rpc_host=rpc_host,\n",
    "    rpc_port=rpc_port\n",
    ")\n",
    "\n",
    "# Set an input\n",
    "input_data = get_input_data()\n",
    "runtime_module.set_input(0, input_data)\n",
    "# Run an inference\n",
    "runtime_module.run()\n",
    "# Get a result\n",
    "out = runtime_module.get_output(0)\n",
    "print(imagenet_1000_class(np.argmax(out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeModule can also run `benchmark` on remote devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 9.475076675415039, 'std': 5.557855606079102, 'max': 26.13087272644043, 'min': 7.470649242401123}\n"
     ]
    }
   ],
   "source": [
    "print(runtime_module.benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeClient locks the server when initialization to block other clients from connecting to the RuntimeServer, and unlocks the server when the RuntimeClient instance is deleted or when the `finalize` method is called.\n",
    "\n",
    "**Only one client can be connected to one Server at the same time. Using a client in the loop of a data loader running in multiprocess may cause gRPC communication to fail.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_module.finalize()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fafabb367c689809e9098d9e06eeddad4dedf0748e439d9c922f0a4231fa874d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
